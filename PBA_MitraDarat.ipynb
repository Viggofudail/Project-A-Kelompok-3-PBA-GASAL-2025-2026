{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Acquisition ##"
      ],
      "metadata": {
        "id": "oW-8ulwwf1uM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feedparser pandas datetime urllib3 sastrawi newspaper3k lxml_html_clean requests_html feedparser requests beautifulsoup4 newspaper3k openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjMyHeRUgXWv",
        "outputId": "dd94093c-8085-4c9f-baac-e5036b246cf7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.12/dist-packages (6.0.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: datetime in /usr/local/lib/python3.12/dist-packages (5.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (1.26.20)\n",
            "Requirement already satisfied: sastrawi in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.12/dist-packages (0.2.8)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.12/dist-packages (0.4.2)\n",
            "Requirement already satisfied: requests_html in /usr/local/lib/python3.12/dist-packages (0.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.12/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.12/dist-packages (from datetime) (8.0.1)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (11.3.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (1.3.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (5.4.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (5.3.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: pyquery in /usr/local/lib/python3.12/dist-packages (from requests_html) (2.0.1)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.12/dist-packages (from requests_html) (2.2.0)\n",
            "Requirement already satisfied: parse in /usr/local/lib/python3.12/dist-packages (from requests_html) (1.20.2)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.12/dist-packages (from requests_html) (0.0.2)\n",
            "Requirement already satisfied: w3lib in /usr/local/lib/python3.12/dist-packages (from requests_html) (2.3.1)\n",
            "Requirement already satisfied: pyppeteer>=0.0.14 in /usr/local/lib/python3.12/dist-packages (from requests_html) (2.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.12/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.12/dist-packages (from pyppeteer>=0.0.14->requests_html) (8.7.0)\n",
            "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in /usr/local/lib/python3.12/dist-packages (from pyppeteer>=0.0.14->requests_html) (11.1.1)\n",
            "Requirement already satisfied: websockets<11.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from pyppeteer>=0.0.14->requests_html) (10.4)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.12/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.19.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html) (3.23.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape Link Berita"
      ],
      "metadata": {
        "id": "IoPyPTp4qnEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sheets berisi Link asli berita (bukan redirect dari google news) : https://docs.google.com/spreadsheets/d/1y-0uxDuZfzFd6bABieboryQ-nXHvE7Xq1WI0gMWurJM/edit?usp=sharing"
      ],
      "metadata": {
        "id": "BxrQ7XK1tpkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from urllib.parse import quote\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def scrape_google_news(keyword):\n",
        "    # Encode keyword biar aman di URL\n",
        "    encoded_keyword = quote(keyword)\n",
        "    rss_url = f\"https://news.google.com/rss/search?q={encoded_keyword}&hl=id&gl=ID&ceid=ID:id\"\n",
        "\n",
        "    feed = feedparser.parse(rss_url)\n",
        "\n",
        "    data = []\n",
        "    for entry in feed.entries:\n",
        "        data.append({\n",
        "            \"title\": entry.title,\n",
        "            \"link\": entry.link,\n",
        "            \"published\": entry.published if \"published\" in entry else None,\n",
        "            \"source\": entry.source.title if \"source\" in entry else None\n",
        "        })\n",
        "\n",
        "    return data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    keyword = '\"Mitra Darat\"'   # gunakan frasa dengan kutip\n",
        "    news_data = scrape_google_news(keyword)\n",
        "\n",
        "    df = pd.DataFrame(news_data)\n",
        "    print(df)\n",
        "\n",
        "    if not df.empty:\n",
        "        filename = f\"news_mitradarat_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
        "        df.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"Data berhasil disimpan di {filename}\")\n",
        "    else:\n",
        "        print(\"Tidak ada berita ditemukan.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_4_LRARf_Gq",
        "outputId": "7affb605-16a9-4e60-b9e0-a2a498c07d95"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                title  \\\n",
            "0   Cara Daftar Mudik Gratis Kemenhub 2025 melalui...   \n",
            "1   Cara Mendaftar Mudik Gratis dari Kemenhub 2025...   \n",
            "2   Ingin Ikut Mudik Gratis 2025? Ini Syarat dan C...   \n",
            "3   Pengguna bisa cek pergerakan Trans Metro Dewat...   \n",
            "4   Syarat dan Cara Daftar Mudik Gratis 2025 Kemen...   \n",
            "..                                                ...   \n",
            "95  Cara Mudik Gratis Pakai Bus, Motor Diangkut Pa...   \n",
            "96  Penambahan Kuota Mudik Gratis dengan Bus Lebar...   \n",
            "97  Cara Daftar Mudik Gratis Lewat Aplikasi MitraD...   \n",
            "98  Cara Dapat Tiket Mudik Gratis 2023 Kemenhub di...   \n",
            "99  Daftar Program Mudik Gratis, Persyaratan dan L...   \n",
            "\n",
            "                                                 link  \\\n",
            "0   https://news.google.com/rss/articles/CBMitwFBV...   \n",
            "1   https://news.google.com/rss/articles/CBMigwFBV...   \n",
            "2   https://news.google.com/rss/articles/CBMidkFVX...   \n",
            "3   https://news.google.com/rss/articles/CBMipwFBV...   \n",
            "4   https://news.google.com/rss/articles/CBMiqgFBV...   \n",
            "..                                                ...   \n",
            "95  https://news.google.com/rss/articles/CBMilwFBV...   \n",
            "96  https://news.google.com/rss/articles/CBMisgFBV...   \n",
            "97  https://news.google.com/rss/articles/CBMivgFBV...   \n",
            "98  https://news.google.com/rss/articles/CBMipgFBV...   \n",
            "99  https://news.google.com/rss/articles/CBMivAFBV...   \n",
            "\n",
            "                        published             source  \n",
            "0   Tue, 18 Mar 2025 07:00:00 GMT           Kumparan  \n",
            "1   Fri, 07 Mar 2025 03:42:41 GMT               UMSU  \n",
            "2   Thu, 13 Feb 2025 08:00:00 GMT  Sahabat Pegadaian  \n",
            "3   Thu, 17 Apr 2025 07:00:00 GMT   ANTARA News Bali  \n",
            "4   Wed, 19 Feb 2025 08:00:00 GMT             Disway  \n",
            "..                            ...                ...  \n",
            "95  Tue, 05 Mar 2024 08:00:00 GMT           detikOto  \n",
            "96  Sat, 30 Mar 2024 07:00:00 GMT          detikNews  \n",
            "97  Wed, 06 Mar 2024 08:00:00 GMT     Tribunnews.com  \n",
            "98  Wed, 15 Mar 2023 07:00:00 GMT          Suara.com  \n",
            "99  Thu, 14 Mar 2024 07:00:00 GMT     CNBC Indonesia  \n",
            "\n",
            "[100 rows x 4 columns]\n",
            "Data berhasil disimpan di news_mitradarat_20250930.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrapping isi berita"
      ],
      "metadata": {
        "id": "dgdcEXmKqbIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_article(url):\n",
        "    try:\n",
        "        # Gunakan newspaper3k lebih dulu\n",
        "        article = Article(url, language=\"id\")\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        return article.title, article.text\n",
        "    except:\n",
        "        try:\n",
        "            # Fallback pakai BeautifulSoup\n",
        "            response = requests.get(url, timeout=10)\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            # Judul\n",
        "            title = soup.find(\"title\").get_text(strip=True) if soup.find(\"title\") else None\n",
        "\n",
        "            # Konten\n",
        "            paragraphs = soup.find_all(\"p\")\n",
        "            text = \" \".join([p.get_text() for p in paragraphs])\n",
        "            return title, text.strip()\n",
        "        except:\n",
        "            return None, None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Baca file Excel (input link berita)\n",
        "    df = pd.read_excel(\"Kelompok 3 - Link Berita MitraDarat.xlsx\")\n",
        "\n",
        "    required_cols = [\"PIC\", \"URL\", \"URL [Direct]\", \"Published\"]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        raise ValueError(f\"File harus memiliki kolom: {required_cols}\")\n",
        "\n",
        "    titles, contents, sources = [], [], []\n",
        "\n",
        "    for link in df[\"URL [Direct]\"]:\n",
        "        title, content = scrape_article(link)\n",
        "        titles.append(title)\n",
        "        contents.append(content)\n",
        "\n",
        "        try:\n",
        "            domain = urlparse(link).netloc\n",
        "        except:\n",
        "            domain = None\n",
        "        sources.append(domain)\n",
        "\n",
        "    # Buat dataframe hasil scraping\n",
        "    scraped_df = pd.DataFrame({\n",
        "        \"title\": titles,\n",
        "        \"link\": df[\"URL [Direct]\"],\n",
        "        \"published\": df[\"Published\"],\n",
        "        \"source\": sources,\n",
        "        \"content_raw\": contents\n",
        "    })\n",
        "\n",
        "    # Simpan hasil scraping (tanpa preprocessing dulu)\n",
        "    output_file = \"news_mitradarat_scraped.csv\"\n",
        "    scraped_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(f\"✅ Data hasil scraping disimpan ke {output_file} (total: {len(scraped_df)})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enX9-bMKZlaK",
        "outputId": "02747a57-c3d5-4a7e-c47e-7153e0a4231d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data hasil scraping disimpan ke news_mitradarat_scraped.csv (total: 174)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "l3jmcylPfo5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "import nltk\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "\n",
        "# Download resource NLTK (sekali saja)\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "# ============== PREPROCESSING ==============\n",
        "def preprocess_text(text):\n",
        "    if not text or not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Hapus karakter non-alfabet\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
        "\n",
        "    # Tokenizing\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Stopwords removal (Indonesian + English)\n",
        "    stop_words = set(stopwords.words(\"indonesian\") + stopwords.words(\"english\"))\n",
        "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "\n",
        "    # Stemming (Sastrawi untuk bahasa Indonesia)\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# ============== MAIN ==============\n",
        "if __name__ == \"__main__\":\n",
        "    # Baca hasil scraping\n",
        "    df = pd.read_csv(\"news_mitradarat_scraped.csv\")\n",
        "\n",
        "    # Preprocessing isi berita\n",
        "    df[\"content_clean\"] = df[\"content_raw\"].apply(preprocess_text)\n",
        "\n",
        "    # Simpan hasil akhir\n",
        "    output_file = \"news_mitradarat_preprocessed.csv\"\n",
        "    df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(f\"✅ Data hasil preprocessing disimpan ke {output_file} (total: {len(df)})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR6ILu9Uq43h",
        "outputId": "bddecdaf-f5cd-4482-d0cf-6dd50a0c506e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"content_clean\"]"
      ],
      "metadata": {
        "id": "qs2ZR2B9t6B9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}